{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitaicondac0e8880bb07b42c396c674eb5a2d8b93",
   "display_name": "Python 3.7.6 64-bit ('ai': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Basic Environment\n",
    "## Step 1: Importing Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "INFO:mlagents_envs:Connected new brain:\nPushBlock?team=0\nINFO:gym_unity:32 agents within environment.\n"
    }
   ],
   "source": [
    "ENV_PATH = '../unity_envs/PushBlock/'\n",
    "ENV_NAME = 'Unity Environment'\n",
    "env = UnityEnv(ENV_PATH + ENV_NAME, worker_id=0, multiagent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Examine the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Agent observations look like: \n[0.         0.         0.         1.         1.         0.\n 0.         0.         1.         1.         0.         0.\n 0.         1.         1.         0.         0.         0.\n 1.         1.         0.         0.         0.         1.\n 1.         0.         0.         0.         1.         1.\n 0.         0.         0.         1.         1.         0.\n 0.         0.         1.         1.         0.         0.\n 0.         1.         1.         0.         0.         0.\n 1.         1.         0.         0.         0.         1.\n 1.         0.         0.         0.         1.         1.\n 0.         0.         0.         1.         1.         0.\n 0.         0.         1.         1.         0.         0.\n 0.         1.         1.         0.         0.         0.\n 1.         1.         0.         0.         0.         1.\n 1.         0.         0.         0.         1.         1.\n 0.         0.         0.         1.         1.         0.\n 0.         0.         1.         1.         0.         0.\n 0.         1.         1.         0.         1.         0.\n 0.         0.8663542  0.         0.         0.         1.\n 1.         0.         0.         0.         1.         1.\n 0.         0.         0.         1.         1.         0.\n 0.         1.         0.         0.8313125  0.         0.\n 0.         1.         1.         0.         0.         1.\n 0.         0.7199376  0.         1.         0.         0.\n 0.7395237  0.         1.         0.         0.         0.8539287\n 0.         1.         0.         0.         0.85392886 0.\n 0.         0.         1.         1.         0.         0.\n 0.         1.         1.         0.         0.         0.\n 1.         1.         0.         0.         1.         0.\n 0.99636906 1.         0.         0.         0.         0.3370348\n 0.         0.         0.         1.         1.         0.\n 0.         0.         1.         1.         0.         0.\n 0.         1.         1.         0.         0.         1.\n 0.         0.71754366 0.         0.         0.         1.\n 1.         0.         0.         1.         0.         0.6214111 ]\n"
    }
   ],
   "source": [
    "# Reset the environment\n",
    "initial_observations = env.reset()\n",
    "\n",
    "if len(env.observation_space.shape) == 1:\n",
    "    # Examine the initial vector observation\n",
    "    print(\"Agent observations look like: \\n{}\".format(initial_observations[0]))\n",
    "else:\n",
    "    # Examine the initial visual observation\n",
    "    print(\"Agent observations look like:\")\n",
    "    if env.observation_space.shape[2] == 3:\n",
    "        plt.imshow(initial_observations[0][:,:,:])\n",
    "    else:\n",
    "        plt.imshow(initial_observations[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Take random steps in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "INFO:mlagents_envs:Environment shut down with return code 0 (CTRL_C_EVENT).\n"
    }
   ],
   "source": [
    "env.reset()\n",
    "score = 0\n",
    "episode_rewards = 0\n",
    "for episode in range(100):\n",
    "    actions = [env.action_space.sample() for agent in range(env.number_agents)]\n",
    "    states, rewards, dones, _ = env.step(actions)\n",
    "    episode_rewards += np.mean(rewards)\n",
    "    if np.any(dones):\n",
    "        print(\"Total reward this episode: {.3f}\".format(episode_rewards))\n",
    "        break\n",
    "env.close()"
   ]
  }
 ]
}